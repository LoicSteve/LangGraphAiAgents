{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "1. Set up a LangGraph workflow\n",
    "2. Define state and nodes for email processing\n",
    "3. Create conditional branching in a graph\n",
    "4. Connect an LLM for classification and content generation\n",
    "5. Visualize the workflow graph\n",
    "6. Execute the workflow with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Using cached langgraph-0.3.30-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
      "  Using cached langchain_core-0.3.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
      "  Using cached langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Using cached langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain_openai)\n",
      "  Using cached openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached langsmith-0.3.31-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.4,>=0.1->langgraph)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./langenv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./langenv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
      "Collecting pydantic<3.0.0,>=2.5.2 (from langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
      "  Downloading ormsgpack-1.9.1-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.25.2 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading orjson-3.10.16-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Using cached langgraph-0.3.30-py3-none-any.whl (144 kB)\n",
      "Using cached langchain_openai-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached langchain_core-0.3.52-py3-none-any.whl (433 kB)\n",
      "Using cached langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
      "Using cached langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
      "Using cached langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
      "Using cached openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl (31 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.3.31-py3-none-any.whl (358 kB)\n",
      "Downloading orjson-3.10.16-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (382 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.8/382.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.6/184.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.0/195.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl (788 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.7/788.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: zstandard, xxhash, urllib3, typing-inspection, tqdm, tenacity, sniffio, regex, PyYAML, pydantic-core, ormsgpack, orjson, jsonpointer, jiter, idna, h11, distro, charset-normalizer, certifi, annotated-types, requests, pydantic, jsonpatch, httpcore, anyio, tiktoken, requests-toolbelt, httpx, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain_openai, langgraph-prebuilt, langgraph\n",
      "Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 distro-1.9.0 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 idna-3.10 jiter-0.9.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.52 langchain_openai-0.3.13 langgraph-0.3.30 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 langsmith-0.3.31 openai-1.75.0 orjson-3.10.16 ormsgpack-1.9.1 pydantic-2.11.3 pydantic-core-2.33.1 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.9.0 tqdm-4.67.1 typing-inspection-0.4.0 urllib3-2.4.0 xxhash-3.5.0 zstandard-0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "%pip install langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Our Environment\n",
    "First, let's import all the necessary libraries. LangGraph provides the graph structure, while LangChain offers convenient interfaces for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize our LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Our State\n",
    "In LangGraph, State is the central concept. It represents all the information that flows through our workflow.\n",
    "\n",
    "For Alfred's email processing system, we need to track:\n",
    "\n",
    "The email being processed\n",
    "Whether it's spam or not\n",
    "The draft response (for legitimate emails)\n",
    "Conversation history with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailState(TypedDict):\n",
    "    # The email being processed\n",
    "    email: Dict[str, Any]  # Contains subject, sender, body, etc.\n",
    "\n",
    "    # Category of the email (inquiry, complaint, etc.)\n",
    "    email_category: Optional[str]\n",
    "\n",
    "    # Reason why the email was marked as spam\n",
    "    spam_reason: Optional[str]\n",
    "\n",
    "    # Analysis and decisions\n",
    "    is_spam: Optional[bool]\n",
    "    \n",
    "    # Response generation\n",
    "    email_draft: Optional[str]\n",
    "    \n",
    "    # Processing metadata\n",
    "    messages: List[Dict[str, Any]]  # Track conversation with LLM for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Our Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# Initialize our LLM\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "def read_email(state: EmailState):\n",
    "    \"\"\"Alfred reads and logs the incoming email\"\"\"\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    # Here we might do some initial preprocessing\n",
    "    print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\")\n",
    "    \n",
    "    # No state changes needed here\n",
    "    return {}\n",
    "\n",
    "def classify_email(state: EmailState):\n",
    "    \"\"\"Alfred uses an LLM to determine if the email is spam or legitimate\"\"\"\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    # Prepare our prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    As Alfred the butler, analyze this email and determine if it is spam or legitimate.\n",
    "    \n",
    "    Email:\n",
    "    From: {email['sender']}\n",
    "    Subject: {email['subject']}\n",
    "    Body: {email['body']}\n",
    "    \n",
    "    First, determine if this email is spam. If it is spam, explain why.\n",
    "    If it is legitimate, categorize it (inquiry, complaint, thank you, etc.).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the LLM\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Simple logic to parse the response (in a real app, you'd want more robust parsing)\n",
    "    response_text = response.content.lower()\n",
    "    is_spam = \"spam\" in response_text and \"not spam\" not in response_text\n",
    "    \n",
    "    # Extract a reason if it's spam\n",
    "    spam_reason = None\n",
    "    if is_spam and \"reason:\" in response_text:\n",
    "        spam_reason = response_text.split(\"reason:\")[1].strip()\n",
    "    \n",
    "    # Determine category if legitimate\n",
    "    email_category = None\n",
    "    if not is_spam:\n",
    "        categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"]\n",
    "        for category in categories:\n",
    "            if category in response_text:\n",
    "                email_category = category\n",
    "                break\n",
    "    \n",
    "    # Update messages for tracking\n",
    "    new_messages = state.get(\"messages\", []) + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "    \n",
    "    # Return state updates\n",
    "    return {\n",
    "        \"is_spam\": is_spam,\n",
    "        \"spam_reason\": spam_reason,\n",
    "        \"email_category\": email_category,\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def handle_spam(state: EmailState):\n",
    "    \"\"\"Alfred discards spam email with a note\"\"\"\n",
    "    print(f\"Alfred has marked the email as spam. Reason: {state['spam_reason']}\")\n",
    "    print(\"The email has been moved to the spam folder.\")\n",
    "    \n",
    "    # We're done processing this email\n",
    "    return {}\n",
    "\n",
    "def draft_response(state: EmailState):\n",
    "    \"\"\"Alfred drafts a preliminary response for legitimate emails\"\"\"\n",
    "    email = state[\"email\"]\n",
    "    category = state[\"email_category\"] or \"general\"\n",
    "    \n",
    "    # Prepare our prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    As Alfred the butler, draft a polite preliminary response to this email.\n",
    "    \n",
    "    Email:\n",
    "    From: {email['sender']}\n",
    "    Subject: {email['subject']}\n",
    "    Body: {email['body']}\n",
    "    \n",
    "    This email has been categorized as: {category}\n",
    "    \n",
    "    Draft a brief, professional response that Mr. Hugg can review and personalize before sending.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the LLM\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Update messages for tracking\n",
    "    new_messages = state.get(\"messages\", []) + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "    \n",
    "    # Return state updates\n",
    "    return {\n",
    "        \"email_draft\": response.content,\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def notify_mr_hugg(state: EmailState):\n",
    "    \"\"\"Alfred notifies Mr. Hugg about the email and presents the draft response\"\"\"\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
    "    print(f\"Subject: {email['subject']}\")\n",
    "    print(f\"Category: {state['email_category']}\")\n",
    "    print(\"\\nI've prepared a draft response for your review:\")\n",
    "    print(\"-\"*50)\n",
    "    print(state[\"email_draft\"])\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # We're done processing this email\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Our Routing Logic\n",
    "\n",
    "We need a function to determine which path to take after classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_email(state: EmailState) -> str:\n",
    "    \"\"\"Determine the next step based on spam classification\"\"\"\n",
    "    if state[\"is_spam\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"legitimate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the StateGraph and Define Edges\n",
    "\n",
    "Now we connect everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "email_graph = StateGraph(EmailState)\n",
    "\n",
    "# Add nodes\n",
    "email_graph.add_node(\"read_email\", read_email)\n",
    "email_graph.add_node(\"classify_email\", classify_email)\n",
    "email_graph.add_node(\"handle_spam\", handle_spam)\n",
    "email_graph.add_node(\"draft_response\", draft_response)\n",
    "email_graph.add_node(\"notify_mr_hugg\", notify_mr_hugg)\n",
    "\n",
    "# Start the edges\n",
    "email_graph.add_edge(START, \"read_email\")\n",
    "# Add edges - defining the flow\n",
    "email_graph.add_edge(\"read_email\", \"classify_email\")\n",
    "\n",
    "# Add conditional branching from classify_email\n",
    "email_graph.add_conditional_edges(\n",
    "    \"classify_email\",\n",
    "    route_email,\n",
    "    {\n",
    "        \"spam\": \"handle_spam\",\n",
    "        \"legitimate\": \"draft_response\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add the final edges\n",
    "email_graph.add_edge(\"handle_spam\", END)\n",
    "email_graph.add_edge(\"draft_response\", \"notify_mr_hugg\")\n",
    "email_graph.add_edge(\"notify_mr_hugg\", END)\n",
    "\n",
    "# Compile the graph\n",
    "compiled_graph = email_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing legitimate email...\n",
      "Alfred is processing an email from john.smith@example.com with subject: Question about your services\n",
      "\n",
      "==================================================\n",
      "Sir, you've received an email from john.smith@example.com.\n",
      "Subject: Question about your services\n",
      "Category: inquiry\n",
      "\n",
      "I've prepared a draft response for your review:\n",
      "--------------------------------------------------\n",
      "Dear John Smith,\n",
      "\n",
      "Thank you for reaching out and expressing interest in our consulting services. I would be happy to schedule a call with you next week to discuss how we can assist you. Please let me know your availability so we can find a suitable time to connect.\n",
      "\n",
      "Best regards,\n",
      "Alfred\n",
      "==================================================\n",
      "\n",
      "\n",
      "Processing spam email...\n",
      "Alfred is processing an email from winner@lottery-intl.com with subject: YOU HAVE WON $5,000,000!!!\n",
      "Alfred has marked the email as spam. Reason: None\n",
      "The email has been moved to the spam folder.\n"
     ]
    }
   ],
   "source": [
    "# Example legitimate email\n",
    "legitimate_email = {\n",
    "    \"sender\": \"john.smith@example.com\",\n",
    "    \"subject\": \"Question about your services\",\n",
    "    \"body\": \"Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith\"\n",
    "}\n",
    "\n",
    "# Example spam email\n",
    "spam_email = {\n",
    "    \"sender\": \"winner@lottery-intl.com\",\n",
    "    \"subject\": \"YOU HAVE WON $5,000,000!!!\",\n",
    "    \"body\": \"CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100.\"\n",
    "}\n",
    "\n",
    "# Process the legitimate email\n",
    "print(\"\\nProcessing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke({\n",
    "    \"email\": legitimate_email,\n",
    "    \"is_spam\": None,\n",
    "    \"spam_reason\": None,\n",
    "    \"email_category\": None,\n",
    "    \"email_draft\": None,\n",
    "    \"messages\": []\n",
    "})\n",
    "\n",
    "# Process the spam email\n",
    "print(\"\\nProcessing spam email...\")\n",
    "spam_result = compiled_graph.invoke({\n",
    "    \"email\": spam_email,\n",
    "    \"is_spam\": None,\n",
    "    \"spam_reason\": None,\n",
    "    \"email_category\": None,\n",
    "    \"email_draft\": None,\n",
    "    \"messages\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing legitimate email...\n",
      "Alfred is processing an email from Joker with subject: Found you Batman ! \n",
      "\n",
      "==================================================\n",
      "Sir, you've received an email from Joker.\n",
      "Subject: Found you Batman ! \n",
      "Category: None\n",
      "\n",
      "I've prepared a draft response for your review:\n",
      "--------------------------------------------------\n",
      "Dear Joker,\n",
      "\n",
      "Thank you for reaching out. I appreciate your interest in my identity, but I must respectfully deny any claims that I am Batman. I assure you, there must be some misunderstanding. \n",
      "\n",
      "If there is anything else you would like to discuss, please feel free to reach out.\n",
      "\n",
      "Sincerely,\n",
      "Mr. Wayne\n",
      "==================================================\n",
      "\n",
      "\n",
      "Processing spam email...\n",
      "Alfred is processing an email from Crypto bro with subject: The best investment of 2025\n",
      "Alfred has marked the email as spam. Reason: None\n",
      "The email has been moved to the spam folder.\n"
     ]
    }
   ],
   "source": [
    " # Example emails for testing\n",
    "legitimate_email = {\n",
    "    \"sender\": \"Joker\",\n",
    "    \"subject\": \"Found you Batman ! \",\n",
    "    \"body\": \"Mr. Wayne,I found your secret identity ! I know you're batman ! Ther's no denying it, I have proof of that and I'm coming to find you soon. I'll get my revenge. JOKER\"\n",
    "}\n",
    "\n",
    "spam_email = {\n",
    "    \"sender\": \"Crypto bro\",\n",
    "    \"subject\": \"The best investment of 2025\",\n",
    "    \"body\": \"Mr Wayne, I just launched an ALT coin and want you to buy some !\"\n",
    "}\n",
    "# Process legitimate email\n",
    "print(\"\\nProcessing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke({\n",
    "    \"email\": legitimate_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "})\n",
    "\n",
    "# Process spam email\n",
    "print(\"\\nProcessing spam email...\")\n",
    "spam_result = compiled_graph.invoke({\n",
    "    \"email\": spam_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"  \" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \" \"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred is processing an email from Joker with subject: Found you Batman ! \n",
      "\n",
      "==================================================\n",
      "Sir, you've received an email from Joker.\n",
      "Subject: Found you Batman ! \n",
      "Category: None\n",
      "\n",
      "I've prepared a draft response for your review:\n",
      "--------------------------------------------------\n",
      "Dear Joker,\n",
      "\n",
      "Thank you for reaching out. I appreciate your interest in my identity, but I must respectfully ask that you refrain from any further attempts to contact me. \n",
      "\n",
      "Sincerely,\n",
      "Alfred\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Process legitimate email\n",
    "legitimate_result = compiled_graph.invoke(\n",
    "    input={\"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None, \"messages\": []},\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred is processing an email from Crypto bro with subject: The best investment of 2025\n",
      "Alfred has marked the email as spam. Reason: None\n",
      "The email has been moved to the spam folder.\n"
     ]
    }
   ],
   "source": [
    "spam_email = {\n",
    "    \"sender\": \"Crypto bro\",\n",
    "    \"subject\": \"The best investment of 2025\",\n",
    "    \"body\": \"Mr Wayne, I just launched an ALT coin and want you to buy some !\"\n",
    "}\n",
    "\n",
    "spam_result = compiled_graph.invoke(\n",
    "    input={\"email\": spam_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None, \"messages\": []},\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Example emails for testing\n",
    "legitimate_email = {\n",
    "    \"sender\": \"Joker\",\n",
    "    \"subject\": \"Found you Batman ! \",\n",
    "    \"body\": \"Mr. Wayne,I found your secret identity ! I know you're batman ! Ther's no denying it, I have proof of that and I'm coming to find you soon. I'll get my revenge. JOKER\"\n",
    "}\n",
    "\n",
    "spam_email = {\n",
    "    \"sender\": \"Crypto bro\",\n",
    "    \"subject\": \"The best investment of 2025\",\n",
    "    \"body\": \"Mr Wayne, I just launched an ALT coin and want you to buy some !\"\n",
    "}\n",
    "# Process legitimate email\n",
    "print(\"\\nProcessing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke({\n",
    "    \"email\": legitimate_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "})\n",
    "\n",
    "# Process spam email\n",
    "print(\"\\nProcessing spam email...\")\n",
    "spam_result = compiled_graph.invoke({\n",
    "    \"email\": spam_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraphAgents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
